\documentclass{beamer}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{comment}
\usetheme{Madrid}

\title[]{Cumulative Reasoning with Large Language Models}
\subtitle{by Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao}
\author{Peer Niklas Sch√§fer}
\institute{University of Cologne}
\date{18.06.2025}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

%-------------------------------

\begin{frame}{Motivation}
    \begin{itemize}
        \item Large Language Models (LLMs) struggle with complex, multi-step reasoning tasks.
        \item Human reasoning often involves iterative validation and composition of intermediate insights.
        \item Cumulative Reasoning (CR) mimics this strategy to enhance LLM performance.
    \end{itemize}
\end{frame}

%-------------------------------

\begin{frame}{Chain of Thought (CoT)}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Prompts the model to generate intermediate steps.
                \item Imitates step-wise human reasoning.
                \item Improves accuracy over direct prompting.
                \item Drawback: single-path, linear structure.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}[htbp]
                \centering
                \includegraphics[width=0.9\textwidth]{../Images/CoT.png}

                \vspace{0.5em}
                {\tiny Wei et al. (2022) NeurIPS, p. 1}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

%-------------------------------

\begin{frame}{Cumulative Reasoning (CR)}
    \begin{itemize}
        \item Introduces three distinct roles:
              \begin{itemize}
                  \item \textbf{Proposer}: Suggests reasoning steps.
                  \item \textbf{Verifier(s)}: Validate steps using logic or tools.
                  \item \textbf{Reporter}: Concludes when sufficient evidence exists.
              \end{itemize}
        \item Steps can be reused across different lines of reasoning.
        \item Constructs reasoning as a directed acyclic graph (DAG).
        \item Mimics constructive human problem-solving.
    \end{itemize}
\end{frame}

%-------------------------------

\begin{frame}{CR Overview}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/CR_process_overview.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], p. 4}
    \end{figure}
\end{frame}

%-------------------------------

\begin{frame}{Experimental Setup}
    \begin{itemize}
        \item Models: GPT-3.5, GPT-4, LLaMA-13B, LLaMA-65B.
        \item Implemented using Microsoft Guidance library.
        \item Same underlying LLM for each role.
        \item Prompts tailored to roles (Proposer, Verifier, Reporter).
        \item Datasets: FOLIO (wiki + curated), AutoTNLI, Game of 24, MATH.
    \end{itemize}
\end{frame}

%-------------------------------

\begin{frame}{FOLIO-Wiki}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{../Images/FOLIO_example.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], p. 27}
    \end{figure}
\end{frame}

\begin{frame}{Results - FOLIO Wiki}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/Results_FOLIOWiki.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], p. 7}
    \end{figure}
\end{frame}

%-------------------------------

\begin{frame}{MATH Dataset}
    \begin{figure}
        \centering
        \fbox{\includegraphics[width=0.9\textwidth]{../Images/MATH_example_problem.png}}
    \end{figure}
    \begin{figure}
        \centering
        \fbox{\includegraphics[width=0.9\textwidth]{../Images/MATH_example_gt.png}}

        \vspace{0.5em}
        {\tiny Hendrycks et al. (2021) NeurIPS, p. 7}
    \end{figure}
\end{frame}

\begin{frame}{Results on MATH dataset without code environment}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/Results_MATH_subcategories.png}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/Results_MATH_difficulties.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], pp. 9-10}
    \end{figure}
\end{frame}

\begin{frame}{Python interpreter as verifier}
    \begin{itemize}
        \item For FOLIO both the \textbf{Proposer} and \textbf{Verifier} were LLMs.
        \item A Python interpreter can serve as an optimal and exact verifier for computations.
        \item As long as the \textbf{Proposer} generates a valid Python code, the verifier is optimal.
        \item Authors compared results to similar approaches, namely \textbf{PAL} (Gao et al., 2022) and \textbf{TORA} (Gou et al., 2024)
    \end{itemize}
\end{frame}

\begin{frame}{Program Aided Language Models (PAL)}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.8\textwidth]{../Images/CoT_Prompt_PAL.png}

                \vspace{0.2em}
                {\tiny Wei et al. (2022) NeurIPS}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/PAL_Prompt.png}

                \vspace{0.2em}
                {\tiny Gao et al. (2022) arXiv:2211.10435v2 [cs.CL], p. 2}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Tool-Integrated Reasoning Agent (TORA)}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/TORA.png}

        \vspace{0.2em}
        {\tiny Gou et al. (2024) arXiv:2309.17452v4 [cs.CL], p. 2}
    \end{figure}
\end{frame}

\begin{frame}{Results on MATH dataset with code environment}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/Results_MATH_subcategories_withCodeEnv.png}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/Results_MATH_difficulties_withCodeEnv.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], p. 10}
    \end{figure}
\end{frame}

%-------------------------------

\begin{frame}{Ablation Studies}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{../Images/AblationStudies.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], p. 18}
    \end{figure}
    \begin{itemize}
        \item Removing the verifier greatly reduces performance.
        \item Removing random premise selection also degrades accuracy.
    \end{itemize}
\end{frame}

%-------------------------------

\begin{frame}{Critique and Conclusion}
    \begin{block}{Critique}
        \begin{itemize}
            \item Lack of reasoning for decisions made.
            \item Small variety of models evaluated.
            \item Incomplete comparisons: not all models evaluated on every task.
            \item More extensive ablation studies needed.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        \begin{itemize}
            \item CR outperforms similar approaches like CoT and ToT.
            \item CR also outperforms TORA and PAL using a Python interpreter.
            \item Promising method for enhancing reasoning capabilities but needs more extensive research.
        \end{itemize}
    \end{block}
\end{frame}


%-------------------------------

\begin{frame}{Discussion and Open Questions}
    \begin{itemize}
        \item Do you think that CR (or other similar strategies) using small, fine-tuned and open-source LLMs could outperform larger LLMs or do you think that increasing the capacity of the LLMs is generally better than more complex reasoning methods?
        \item Since autoregressive LLMs are fundamentally trained to predict the next token, does it make sense to keep enhancing their reasoning abilities through prompting strategies like CR? Or should we be investing in fundamentally different model types better suited for reasoning?
    \end{itemize}
\end{frame}

%-------------------------------

\begin{frame}{Results on AutoTNLI and Game of 24}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/Results_AutoTNLI_GameOf24.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], p. 8}
    \end{figure}
\end{frame}

\begin{frame}{Other experimental results}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{../Images/MoreExperiments.png}

        \vspace{0.5em}
        {\tiny Zhang et al. (2023) arXiv:2308.04371v7 [cs.AI], p. 18}
    \end{figure}
\end{frame}

\begin{frame}{Tree of Thought (ToT)}
    \begin{columns}
        \begin{column}{0.5 \textwidth}
            \begin{itemize}
                \item Explores multiple reasoning paths in tree structures.
                \item Uses generators and evaluators for candidate thoughts.
                \item Employs search strategies (e.g., BFS/DFS).
                \item Drawback: High computational cost and inefficiency.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.8\textwidth]{../Images/ToT.png}

                \vspace{0.5em}
                {\tiny Yao et al. (2023) NeurIPS, p. 2}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\end{document}
