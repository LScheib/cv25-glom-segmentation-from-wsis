% This file was created with Citavi 6.19.3.1

@proceedings{.2021,
 year = {2021},
 title = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
 publisher = {IEEE},
 isbn = {978-1-6654-2812-5}
}


@proceedings{.2024,
 year = {2024},
 title = {2024 IEEE International Symposium on Biomedical Imaging (ISBI)},
 publisher = {IEEE},
 isbn = {979-8-3503-1333-8}
}


@misc{Cap.07.11.2024,
 abstract = {Whole-slide images (WSI) glomerulus segmentation is essential for accurately diagnosing kidney diseases. In this work, we propose a general and practical pipeline for glomerulus segmentation that effectively enhances both patch-level and WSI-level segmentation tasks. Our approach leverages stitching on overlapping patches, increasing the detection coverage, especially when glomeruli are located near patch image borders. In addition, we conduct comprehensive evaluations from different segmentation models across two large and diverse datasets with over 30K glomerulus annotations. Experimental results demonstrate that models using our pipeline outperform the previous state-of-the-art method, achieving superior results across both datasets and setting a new benchmark for glomerulus segmentation in WSIs. The code and pre-trained models are available at https://github.com/huuquan1994/wsi{\_}glomerulus{\_}seg.},
 author = {Cap, Quan Huu},
 date = {07.11.2024},
 title = {A General Pipeline for Glomerulus Whole-Slide Image Segmentation},
 doi = {10.48550/arXiv.2411.04782}
}


@misc{Cheng.02.12.2021,
 abstract = {Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).},
 author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
 date = {02.12.2021},
 title = {Masked-attention Mask Transformer for Universal Image Segmentation},
 doi = {10.48550/arXiv.2112.01527},
 urldate = {13.07.2025},
}


@misc{Deng.11.02.2025,
 abstract = {Chronic kidney disease (CKD) is a major global health issue, affecting over 10{\%} of the population and causing significant mortality. While kidney biopsy remains the gold standard for CKD diagnosis and treatment, the lack of comprehensive benchmarks for kidney pathology segmentation hinders progress in the field. To address this, we organized the Kidney Pathology Image Segmentation (KPIs) Challenge, introducing a dataset that incorporates preclinical rodent models of CKD with over 10,000 annotated glomeruli from 60+ Periodic Acid Schiff (PAS)-stained whole slide images. The challenge includes two tasks, patch-level segmentation and whole slide image segmentation and detection, evaluated using the Dice Similarity Coefficient (DSC) and F1-score. By encouraging innovative segmentation methods that adapt to diverse CKD models and tissue conditions, the KPIs Challenge aims to advance kidney pathology analysis, establish new benchmarks, and enable precise, large-scale quantification for disease research and diagnosis.},
 author = {Deng, Ruining and Yao, Tianyuan and Tang, Yucheng and Guo, Junlin and Lu, Siqi and Xiong, Juming and Yu, Lining and Cap, Quan Huu and Cai, Pengzhou and Lan, Libin and Zhao, Ze and Galdran, Adrian and Kumar, Amit and Deotale, Gunjan and Das, Dev Kumar and Paik, Inyoung and Lee, Joonho and Lee, Geongyu and Chen, Yujia and Li, Wangkai and Li, Zhaoyang and Hou, Xuege and Wu, Zeyuan and Wang, Shengjin and Fischer, Maximilian and Kramer, Lars and {Du Anghong} and {Le Zhang} and Sanchez, Maria Sanchez and Ulloa, Helena Sanchez and Heredia, David Ribalta and Garcia, Carlos Perez de Arenaza and Xu, Shuoyu and He, Bingdou and Cheng, Xinping and Wang, Tao and Moreau, Noemie and Bozek, Katarzyna and Innani, Shubham and Baid, Ujjwal and Kefas, Kaura Solomon and Landman, Bennett A. and Wang, Yu and Zhao, Shilin and Yin, Mengmeng and Yang, Haichun and Huo, Yuankai},
 date = {11.02.2025},
 title = {KPIs 2024 Challenge: Advancing Glomerular Segmentation from Patch- to  Slide-Level},
 doi = {10.48550/arXiv.2502.07288}
}


@article{Li.2021,
 abstract = {Purpose: Recent advances in computational image analysis offer the opportunity to develop automatic quantification of histologic parameters as aid tools for practicing pathologists. We aim to develop deep learning (DL) models to quantify nonsclerotic and sclerotic glomeruli on frozen sections from donor kidney biopsies. Approach: A total of 258 whole slide images (WSI) from cadaveric donor kidney biopsies performed at our institution ( n=123 ) and at external institutions ( n=135 ) were used in this study. WSIs from our institution were divided at the patient level into training and validation datasets (ratio: 0.8:0.2), and external WSIs were used as an independent testing dataset. Nonsclerotic ( n=22767 ) and sclerotic ( n=1366 ) glomeruli were manually annotated by study pathologists on all WSIs. A nine-layer convolutional neural network based on the common U-Net architecture was developed and tested for the segmentation of nonsclerotic and sclerotic glomeruli. DL-derived, manual segmentation, and reported glomerular count (standard of care) were compared. Results: The average Dice similarity coefficient testing was 0.90 and 0.83. And the F1 , recall, and precision scores were 0.93, 0.96, and 0.90, and 0.87, 0.93, and 0.81, for nonsclerotic and sclerotic glomeruli, respectively. DL-derived and manual segmentation-derived glomerular counts were comparable, but statistically different from reported glomerular count. Conclusions: DL segmentation is a feasible and robust approach for automatic quantification of glomeruli. We represent the first step toward new protocols for the evaluation of donor kidney biopsies.},
 author = {Li, Xiang and Davis, Richard C. and Xu, Yuemei and Wang, Zehan and Souma, Nao and Sotolongo, Gina and Bell, Jonathan and Ellis, Matthew and Howell, David and Shen, Xiling and Lafata, Kyle J. and Barisoni, Laura},
 year = {2021},
 title = {Deep learning segmentation of glomeruli on kidney donor frozen sections},
 pages = {067501},
 volume = {8},
 number = {6},
 issn = {2329-4302},
 journal = {Journal of medical imaging},
 doi = {10.1117/1.JMI.8.6.067501}
}


@inproceedings{Liu.2021,
 abstract = {2021 IEEE/CVF International Conference on Computer Vision (ICCV);2021; ; ;10.1109/ICCV48922.2021.00986},
 author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
 title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
 pages = {9992--10002},
 publisher = {IEEE},
 isbn = {978-1-6654-2812-5},
 booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
 year = {2021},
 doi = {\url{10.1109/ICCV48922.2021.00986}},
 file = {Swin{\_}Transformer{\_}Hierarchical{\_}Vision{\_}Transformer{\_}using{\_}Shifted{\_}Windows:Attachments/Swin{\_}Transformer{\_}Hierarchical{\_}Vision{\_}Transformer{\_}using{\_}Shifted{\_}Windows.pdf:application/pdf}
}


@inproceedings{Moreau.2024,
 abstract = {2024 IEEE International Symposium on Biomedical Imaging (ISBI);2024; ; ;10.1109/ISBI56570.2024.10635729},
 author = {Moreau, No{\'e}mie and Shabani, Michelle and Schell, Christoph and Bozek, Katarzyna},
 title = {GLOMNET: A Hover Deep Learning Model for Glomerulus Instance Segmentation},
 pages = {1--5},
 publisher = {IEEE},
 isbn = {979-8-3503-1333-8},
 booktitle = {2024 IEEE International Symposium on Biomedical Imaging (ISBI)},
 year = {2024},
 doi = {10.1109/ISBI56570.2024.10635729},
}


@misc{Xiao.26.07.2018,
 abstract = {Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes. Models are available at \url{https://github.com/CSAILVision/unifiedparsing}.},
 author = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
 date = {26.07.2018},
 title = {Unified Perceptual Parsing for Scene Understanding},
 doi = {10.48550/arXiv.1807.10221}
}


@misc{Ronneberger.18.05.2015,
 abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 date = {18.05.2015},
 title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
 doi = {10.48550/arXiv.1505.04597}
}


@misc{Simonyan.04.09.2014,
 abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
 author = {Simonyan, Karen and Zisserman, Andrew},
 date = {04.09.2014},
 title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
 doi = {10.48550/arXiv.1409.1556}
}


% This file was created with Citavi 6.19.3.1

@inproceedings{Gamper.2019,
 abstract = {In this work we present an experimental setup to semi automatically obtain exhaustive nuclei labels across 19 different tissue types, and therefore construct a large pan-cancer dataset for nuclei instance segmentation and classification, with minimal sampling bias....},
 author = {Gamper, Jevgenij and {Alemi Koohbanani}, Navid and Benet, Ksenija and Khuram, Ali and Rajpoot, Nasir},
 title = {PanNuke: An Open Pan-Cancer Histology Dataset for Nuclei Instance Segmentation and Classification},
 pages = {11--19},
 publisher = {Springer},
 isbn = {978-3-030-23937-4},
 series = {Springer eBooks Computer Science},
 editor = {Reyes-Aldasoro, Constantino Carlos and Janowczyk, Andrew and Veta, Mitko and Bankhead, Peter and Sirinukunwattana, Korsuk},
 booktitle = {Digital Pathology},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-23937-4},
 file = {Gamper, Alemi Koohbanani et al. 2019 - PanNuke An Open Pan-Cancer Histology:Attachments/Gamper, Alemi Koohbanani et al. 2019 - PanNuke An Open Pan-Cancer Histology.pdf:application/pdf}
}


@proceedings{ReyesAldasoro.2019,
 abstract = {Image Datasets and Virtual Staining -- Segmentation -- Computer-Assisted Diagnosis and Prognosis



This book constitutes the refereed proceedings of the 15th European Congress on Digital Pathology, ECDP 2019, held in Warwick, UK in April 2019. The 21 full papers presented in this volume were carefully reviewed and selected from 30 submissions. The congress theme will be Accelerating Clinical Deployment, with a focus on computational pathology and leveraging the power of big data and artificial intelligence to bridge the gaps between research, development, and clinical uptake},
 year = {2019},
 title = {Digital Pathology: 15th European Congress, ECDP 2019, Warwick, UK, April 10--13, 2019, Proceedings},
 address = {Cham},
 edition = {1st ed. 2019},
 volume = {11435},
 publisher = {Springer},
 isbn = {978-3-030-23937-4},
 series = {Springer eBooks Computer Science},
 editor = {Reyes-Aldasoro, Constantino Carlos and Janowczyk, Andrew and Veta, Mitko and Bankhead, Peter and Sirinukunwattana, Korsuk},
 doi = {10.1007/978-3-030-23937-4}
}


% This file was created with Citavi 6.19.3.1

@misc{Tang.03.07.2024,
 abstract = {In digital pathology, the traditional method for deep learning-based image segmentation typically involves a two-stage process: initially segmenting high-resolution whole slide images (WSI) into smaller patches (e.g., 256x256, 512x512, 1024x1024) and subsequently reconstructing them to their original scale. This method often struggles to capture the complex details and vast scope of WSIs. In this paper, we propose the holistic histopathology (HoloHisto) segmentation method to achieve end-to-end segmentation on gigapixel WSIs, whose maximum resolution is above 80,000$\times$70,000 pixels. HoloHisto fundamentally shifts the paradigm of WSI segmentation to an end-to-end learning fashion with 1) a large (4K) resolution base patch for elevated visual information inclusion and efficient processing, and 2) a novel sequential tokenization mechanism to properly model the contextual relationships and efficiently model the rich information from the 4K input. To our best knowledge, HoloHisto presents the first holistic approach for gigapixel resolution WSI segmentation, supporting direct I/O of complete WSI and their corresponding gigapixel masks. Under the HoloHisto platform, we unveil a random 4K sampler that transcends ultra-high resolution, delivering 31 and 10 times more pixels than standard 2D and 3D patches, respectively, for advancing computational capabilities. To facilitate efficient 4K resolution dense prediction, we leverage sequential tokenization, utilizing a pre-trained image tokenizer to group image features into a discrete token grid. To assess the performance, our team curated a new kidney pathology image segmentation (KPIs) dataset with WSI-level glomeruli segmentation from whole mouse kidneys. From the results, HoloHisto-4K delivers remarkable performance gains over previous state-of-the-art models.},
 author = {Tang, Yucheng and He, Yufan and Nath, Vishwesh and Guo, Pengfeig and Deng, Ruining and Yao, Tianyuan and Liu, Quan and Cui, Can and Yin, Mengmeng and Xu, Ziyue and Roth, Holger and Xu, Daguang and Yang, Haichun and Huo, Yuankai},
 date = {03.07.2024},
 title = {HoloHisto: End-to-end Gigapixel WSI Segmentation with 4K Resolution  Sequential Tokenization},
 urldate = {13.07.2025},
 doi = {10.48550/arXiv.2407.03307},
 file = {2407:Attachments/2407.pdf:application/pdf}
}


@misc{Wu.21.02.2024,
 abstract = {Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.},
 author = {Wu, Huaqian and Br{\'e}mond-Martin, Clara and Bouaou, K{\'e}vin and Clouchoux, C{\'e}dric},
 date = {21.02.2024},
 title = {Tumor segmentation on whole slide images: training or prompting?},
 urldate = {13.07.2025},
 doi = {10.48550/arXiv.2402.13932},
 file = {2402:Attachments/2402.pdf:application/pdf}
}



@misc{Oktay.11.04.2018,
 abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
 author = {Oktay, Ozan and Schlemper, Jo and {Le Folgoc}, Loic and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
 date = {11.04.2018},
 title = {Attention U-Net: Learning Where to Look for the Pancreas},
 url = {http://arxiv.org/pdf/1804.03999v3},
 urldate = {13.07.2025},
 file = {1804:Attachments/1804.pdf:application/pdf}
}


@misc{Wang.07.04.2020,
 abstract = {This paper proposes a novel U-Net variant using stacked dilated convolutions for medical image segmentation (SDU-Net). SDU-Net adopts the architecture of vanilla U-Net with modifications in the encoder and decoder operations (an operation indicates all the processing for feature maps of the same resolution). Unlike vanilla U-Net which incorporates two standard convolutions in each encoder/decoder operation, SDU-Net uses one standard convolution followed by multiple dilated convolutions and concatenates all dilated convolution outputs as input to the next operation. Experiments showed that SDU-Net outperformed vanilla U-Net, attention U-Net (AttU-Net), and recurrent residual U-Net (R2U-Net) in all four tested segmentation tasks while using parameters around 40{\%} of vanilla U-Net's, 17{\%} of AttU-Net's, and 15{\%} of R2U-Net's.},
 author = {Wang, Shuhang and Hu, Szu-Yeu and Cheah, Eugene and Wang, Xiaohong and Wang, Jingchao and Chen, Lei and Baikpour, Masoud and Ozturk, Arinc and Li, Qian and Chou, Shinn-Huey and Lehman, Constance D. and Kumar, Viksit and Samir, Anthony},
 date = {07.04.2020},
 title = {U-Net Using Stacked Dilated Convolutions for Medical Image Segmentation},
 url = {http://arxiv.org/pdf/2004.03466},
 file = {Wang, Hu et al. 07.04.2020 - U-Net Using Stacked Dilated Convolutions:Attachments/Wang, Hu et al. 07.04.2020 - U-Net Using Stacked Dilated Convolutions.pdf:application/pdf}
}



@misc{Kim.26.07.2023,
 abstract = {DeepLab is a widely used deep neural network for semantic segmentation, whose success is attributed to its parallel architecture called atrous spatial pyramid pooling (ASPP). ASPP uses multiple atrous convolutions with different atrous rates to extract both local and global information. However, fixed values of atrous rates are used for the ASPP module, which restricts the size of its field of view. In principle, atrous rate should be a hyperparameter to change the field of view size according to the target task or dataset. However, the manipulation of atrous rate is not governed by any guidelines. This study proposes practical guidelines for obtaining an optimal atrous rate. First, an effective receptive field for semantic segmentation is introduced to analyze the inner behavior of segmentation networks. We observed that the use of ASPP module yielded a specific pattern in the effective receptive field, which was traced to reveal the module's underlying mechanism. Accordingly, we derive practical guidelines for obtaining the optimal atrous rate, which should be controlled based on the size of input image. Compared to other values, using the optimal atrous rate consistently improved the segmentation results across multiple datasets, including the STARE, CHASE{\_}DB1, HRF, Cityscapes, and iSAID datasets.},
 author = {Kim, Bum Jun and Choi, Hyeyeon and Jang, Hyeonah and Kim, Sang Woo},
 date = {26.07.2023},
 title = {Resolution-Aware Design of Atrous Rates for Semantic Segmentation  Networks},
 url = {https://arxiv.org/pdf/2307.14179},
 urldate = {13.07.2025},
 file = {2307:Attachments/2307.pdf:application/pdf}
}







